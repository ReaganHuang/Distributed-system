{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "from __future__ import print_function\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, Normalizer\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import array, struct, split, explode, udf, col, collect_list\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import concat, col, lit, monotonically_increasing_id\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans, LDA, LDAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyspark_submit_args = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0 pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .config('spark.mongodb.input.uri', \"mongodb://34.220.42.106/monskr.review1\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark2 = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config('spark.mongodb.input.uri', \"mongodb://34.220.42.106/monskr.meta1\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading first dataset corresponding to the reviews\n",
    "review = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the meta dataset including the titles and the description of the books\n",
    "meta = spark2.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_id', 'asin', 'helpful', 'overall', 'reviewText', 'reviewTime', 'reviewerID', 'reviewerName', 'summary', 'unixReviewTime']\n",
      "['asin', 'title', 'description']\n"
     ]
    }
   ],
   "source": [
    "print(review.columns)\n",
    "print(meta.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the data subset coresponding to the 2012 year\n",
    "df2012 = review.where(review.reviewTime.contains('2012'))\n",
    "# extract the data subset coresponding to the 2013 year\n",
    "df2013 = review.where(review.reviewTime.contains('2013'))\n",
    "# extract the data subset coresponding to the 2014 year\n",
    "df2014 = review.where(review.reviewTime.contains('2014'))\n",
    "# Join the three sub-datasets\n",
    "reviews = df2012.union(df2013).union(df2014)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatinating the review Text and the description text together in the review data\n",
    "reviews = reviews.select('asin',\n",
    "                        concat(col(\"reviewText\"),lit(\"\"),col(\"summary\")).alias('textss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selecting the columns needed from meta dataset\n",
    "meta = meta.drop_duplicates(['asin']).select('asin', 'title', 'description') #dropped categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting all the reviews per book_id (asin)\n",
    "reviews_gb = reviews.groupBy(\"asin\").agg(f.concat_ws(\" \", f.collect_list('textss')).alias('textss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Joining the two data sets\n",
    "df = reviews_gb.join(meta, 'asin', 'inner').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Again joining the text columns from the meta data afyer the join\n",
    "df = df.select('asin','title',\n",
    "                concat(col(\"textss\"),lit(\"\"),col(\"description\")).alias('reviewText'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we delete the columns with no texts entry\n",
    "dff = df.na.drop(subset=[\"reviewText\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new cilumn for indexing\n",
    "dff = dff.select(\"*\").withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---+\n",
      "|      asin|               title|          reviewText| id|\n",
      "+----------+--------------------+--------------------+---+\n",
      "|0670869961|     The Pasta Bible|This is a wonderf...|  0|\n",
      "|0671019880|Upon a Midnight C...|This is a book fi...|  1|\n",
      "|0671617478|           Red Baker|I'm not sure how ...|  2|\n",
      "|0671732188|Bestial: The Sava...|Interesting how t...|  3|\n",
      "|0671736450|The Way of Energy...|I can't remember ...|  4|\n",
      "+----------+--------------------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final data set ready for modeling!!\n",
    "dff.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---+--------------------+--------------------+\n",
      "|      asin|               title|          reviewText| id|               words|            mf_words|\n",
      "+----------+--------------------+--------------------+---+--------------------+--------------------+\n",
      "|0670869961|     The Pasta Bible|This is a wonderf...|  0|[this, is, a, won...|[wonderful, book,...|\n",
      "|0671019880|Upon a Midnight C...|This is a book fi...|  1|[this, is, a, boo...|[book, filled, sh...|\n",
      "|0671617478|           Red Baker|I'm not sure how ...|  2|[i'm, not, sure, ...|[sure, (or, why),...|\n",
      "+----------+--------------------+--------------------+---+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Tokenizer and apply it to the data\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\") \n",
    "df_w_words = tokenizer.transform(dff)\n",
    "# Remove the stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"mf_words\")  \n",
    "df_w_mfwords = remover.transform(df_w_words)\n",
    "df_w_mfwords.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|      asin|               title|          reviewText| id|               words|            mf_words|                  tf|            features|\n",
      "+----------+--------------------+--------------------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|0670869961|     The Pasta Bible|This is a wonderf...|  0|[this, is, a, won...|[wonderful, book,...|(200,[0,1,2,3,4,5...|(200,[0,1,2,3,4,5...|\n",
      "|0671019880|Upon a Midnight C...|This is a book fi...|  1|[this, is, a, boo...|[book, filled, sh...|(200,[0,1,3,5,8,1...|(200,[0,1,3,5,8,1...|\n",
      "|0671617478|           Red Baker|I'm not sure how ...|  2|[i'm, not, sure, ...|[sure, (or, why),...|(200,[0,1,2,3,4,5...|(200,[0,1,2,3,4,5...|\n",
      "+----------+--------------------+--------------------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the TFIDF algorithm to get \n",
    "hashingTF = HashingTF(inputCol=\"mf_words\", outputCol=\"tf\", numFeatures=200) #need to know how to choose the numFeatures\n",
    "tf = hashingTF.transform(df_w_mfwords)  # reviews_w_feature  == tf\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"features\").fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "tfidf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- mf_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tf: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " tfidf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# persist the features data frame to save time during clustering\n",
    "tfidf.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\")\n",
    "l2NormData = normalizer.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply kmeans for 10 topics and 50 as maximum iterartin numbers\n",
    "kmeans = KMeans().setK(10).setMaxIter(50)\n",
    "km_model = kmeans.fit(l2NormData)\n",
    "clustersTable = km_model.transform(l2NormData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- mf_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tf: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- normFeatures: vector (nullable = true)\n",
      " |-- prediction: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustersTable.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|      asin|               title|          reviewText| id|               words|            mf_words|                  tf|            features|        normFeatures|prediction|\n",
      "+----------+--------------------+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|0670869961|     The Pasta Bible|This is a wonderf...|  0|[this, is, a, won...|[wonderful, book,...|(200,[0,1,2,3,4,5...|(200,[0,1,2,3,4,5...|(200,[0,1,2,3,4,5...|         0|\n",
      "|0671019880|Upon a Midnight C...|This is a book fi...|  1|[this, is, a, boo...|[book, filled, sh...|(200,[0,1,3,5,8,1...|(200,[0,1,3,5,8,1...|(200,[0,1,3,5,8,1...|         0|\n",
      "|0671617478|           Red Baker|I'm not sure how ...|  2|[i'm, not, sure, ...|[sure, (or, why),...|(200,[0,1,2,3,4,5...|(200,[0,1,2,3,4,5...|(200,[0,1,2,3,4,5...|         0|\n",
      "|0671732188|Bestial: The Sava...|Interesting how t...|  3|[interesting, how...|[interesting, med...|(200,[0,1,2,3,4,5...|(200,[0,1,2,3,4,5...|(200,[0,1,2,3,4,5...|         0|\n",
      "|0671736450|The Way of Energy...|I can't remember ...|  4|[i, can't, rememb...|[remember, exactl...|(200,[1,2,3,4,5,6...|(200,[1,2,3,4,5,6...|(200,[1,2,3,4,5,6...|         0|\n",
      "+----------+--------------------+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustersTable.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  208|\n",
      "|         6| 1367|\n",
      "|         3|   79|\n",
      "|         5|  582|\n",
      "|         9|   42|\n",
      "|         4|   17|\n",
      "|         8|13413|\n",
      "|         7|   11|\n",
      "|         2| 3951|\n",
      "|         0|60931|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the number of books per topic for k=10\n",
    "clustersTable.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aplly kmeans with 6 categories\n",
    "kmeans = KMeans().setK(6).setMaxIter(50)\n",
    "km_model = kmeans.fit(l2NormData)\n",
    "clustersTable = km_model.transform(l2NormData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  192|\n",
      "|         3|   20|\n",
      "|         5|11848|\n",
      "|         4| 3007|\n",
      "|         2|  893|\n",
      "|         0|64641|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the number of books per topic for k=6\n",
    "clustersTable.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply LDA for k=10\n",
    "lda = LDA(k=10)\n",
    "model = lda.fit(l2NormData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|topic|count|\n",
      "+-----+-----+\n",
      "|    0|  973|\n",
      "|    1|  809|\n",
      "|    2| 1178|\n",
      "|    3|  871|\n",
      "|    4| 1046|\n",
      "|    5| 1009|\n",
      "|    6| 1297|\n",
      "|    7|  733|\n",
      "|    8|  878|\n",
      "|    9|  632|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the number of books per topic for k=10\n",
    "model.describeTopics().rdd.map(lambda x: (x[0],sum(x[1]))).toDF().withColumnRenamed(\"_1\",\"topic\").withColumnRenamed(\"_2\",\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply LDA for k=6\n",
    "lda = LDA(k=6)\n",
    "model = lda.fit(l2NormData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|topic|count|\n",
      "+-----+-----+\n",
      "|    0|  906|\n",
      "|    1|  819|\n",
      "|    2| 1171|\n",
      "|    3|  869|\n",
      "|    4| 1046|\n",
      "|    5| 1066|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the number of books per topic for k=6\n",
    "model.describeTopics().rdd.map(lambda x: (x[0],sum(x[1]))).toDF().withColumnRenamed(\"_1\",\"topic\").withColumnRenamed(\"_2\",\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(200, 10, [4447.7533, 4066.6514, 6946.4871, 2954.0353, 2727.0803, 3766.1738, 3913.7805, 3815.6799, ..., 3141.578, 1696.8065, 3513.166, 3303.174, 2629.1893, 2707.0874, 3013.417, 3145.8837], 0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
